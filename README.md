Multi-Threaded Web Crawler

A Python-based multi-threaded web crawler that efficiently fetches and stores URLs while supporting resume functionality. It uses threading for parallel crawling and SQLite for persistence.

ğŸš€ Features

Multi-threaded crawling for faster performance

Resume from last crawl using SQLite database

Simple, modular design (easy to extend)

Graceful handling of invalid or duplicate URLs

ğŸ§  How It Works

Fetches the starting URL and extracts all valid links.

Each thread handles a subset of URLs concurrently.

Visited links are stored in a local database to avoid repetition.

Crawl can resume later using saved data.

âš™ï¸ Usage
pip install -r requirements.txt
python run.py https://example.com

ğŸ“‚ Project Structure
crawler/
â”œâ”€â”€ main.py        # Entry point
â”œâ”€â”€ worker.py      # Thread logic
â”œâ”€â”€ database.py    # SQLite operations
â”œâ”€â”€ fetcher.py     # Fetch & parse links
â””â”€â”€ utils.py       # Helper functions